# 🎬 Machine Learning Text Classifier – Sprint 14: IMDB Review Sentiment Analysis

A Natural Language Processing (NLP) project built to help the **Film Junky Union** automatically filter negative movie reviews. Using a labeled dataset of IMDB reviews, this project demonstrates the end-to-end development of a binary sentiment classifier.

---

## 📌 Project Description

The objective of this project was to train a machine learning model capable of detecting **negative** movie reviews with high precision and recall. The goal was to achieve an **F1 score of at least 0.85**.

The final model classifies user-generated reviews as either **positive** or **negative**, enabling streamlined feedback filtering for Film Junky Union moderators.

---

## 📂 Dataset

- Source: `imdb_reviews.tsv`
- Contains IMDB movie reviews labeled by **sentiment polarity** (positive or negative).
- Balanced dataset with roughly equal representation of both classes.

---

## 📊 Key Steps

1. **Data Loading and Preprocessing**
   - Loaded the data from `imdb_reviews.tsv`.
   - Preprocessed text data: lowercasing, tokenization, and removing noise.

2. **Exploratory Data Analysis (EDA)**
   - Visualized class distribution to check for imbalance.
   - Analyzed review length and word frequency.
   - Identified key patterns in sentiment-bearing terms.

3. **Feature Engineering**
   - Converted text into numerical format using:
     - **TF-IDF vectorization**
     - Optional: Bag-of-Words for comparison.

4. **Model Training**
   - Trained three different models:
     - Logistic Regression
     - Random Forest Classifier
     - Multinomial Naive Bayes
   - Used GridSearchCV for hyperparameter tuning.

5. **Model Evaluation**
   - Evaluated models using:
     - F1 Score
     - Accuracy
     - Confusion Matrix
   - **Best model:** Logistic Regression (TF-IDF), with **F1 Score ≈ 0.87**

6. **Custom Review Classification**
   - Input several custom reviews to test model behavior.
   - Compared model predictions for consistency and edge-case handling.

7. **Results & Discussion**
   - Logistic Regression outperformed other models, particularly in recall.
   - TF-IDF significantly boosted classification accuracy.
   - Minor misclassifications often occurred with sarcastic or ambiguous reviews.

---

## 💡 Insights

- **Logistic Regression** emerged as the top-performing model, striking a balance between accuracy, precision, and recall — particularly effective with TF-IDF features.
- **TF-IDF vectorization** proved superior to Bag-of-Words by reducing the impact of frequently occurring, less informative words and amplifying rare but sentiment-rich terms.
- **Multinomial Naive Bayes** was fast and interpretable but occasionally overfit common sentiment cues, making it less reliable on subtle or sarcastic text.
- **Random Forest** underperformed compared to expectations — tree-based models often struggle with high-dimensional, sparse matrices generated by vectorized text.
- **Review length and vocabulary richness** didn’t always correlate with classification accuracy; short but emotionally charged phrases (e.g., “Hated it.” or “Loved every second.”) were easier to classify correctly.
- **Custom reviews** helped surface model edge cases. For instance, reviews with sarcasm or mixed sentiment (e.g., "I wish I could forget this masterpiece") challenged all models and sometimes led to false positives/negatives.
- The **confusion matrix** revealed that false negatives (failing to catch a bad review) were more common than false positives, which may be undesirable depending on the business goal (e.g., filtering negativity).
- Future improvements could involve:
  - Incorporating word embeddings (e.g., Word2Vec or BERT) for better contextual understanding.
  - Leveraging deep learning approaches like LSTM or transformers to handle complex language constructs.
  - Implementing sentiment-specific lexicons or ensemble strategies to improve accuracy on nuanced reviews.

---

## 🛠️ Tools Used

- Python
- Pandas, NumPy
- scikit-learn
- NLTK
- Matplotlib, Seaborn
- Jupyter Notebook

---

## 🧠 Skills Applied

`Natural Language Processing (NLP)` · `Text Preprocessing` · `TF-IDF` · `Classification Metrics` · `Logistic Regression` · `Random Forest` · `Naive Bayes` · `Model Evaluation` · `Sentiment Analysis`

