# ğŸ¬ Machine Learning Text Classifier â€“ Sprint 14: IMDB Review Sentiment Analysis

A Natural Language Processing (NLP) project built to help the **Film Junky Union** automatically filter negative movie reviews. Using a labeled dataset of IMDB reviews, this project demonstrates the end-to-end development of a binary sentiment classifier.

---

## ğŸ“Œ Project Description

The objective of this project was to train a machine learning model capable of detecting **negative** movie reviews with high precision and recall. The goal was to achieve an **F1 score of at least 0.85**.

The final model classifies user-generated reviews as either **positive** or **negative**, enabling streamlined feedback filtering for Film Junky Union moderators.

---

## ğŸ“‚ Dataset

- Source: `imdb_reviews.tsv`
- Contains IMDB movie reviews labeled by **sentiment polarity** (positive or negative).
- Balanced dataset with roughly equal representation of both classes.

---

## ğŸ“Š Key Steps

1. **Data Loading and Preprocessing**
   - Loaded the data from `imdb_reviews.tsv`.
   - Preprocessed text data: lowercasing, tokenization, and removing noise.

2. **Exploratory Data Analysis (EDA)**
   - Visualized class distribution to check for imbalance.
   - Analyzed review length and word frequency.
   - Identified key patterns in sentiment-bearing terms.

3. **Feature Engineering**
   - Converted text into numerical format using:
     - **TF-IDF vectorization**
     - Optional: Bag-of-Words for comparison.

4. **Model Training**
   - Trained three different models:
     - Logistic Regression
     - Random Forest Classifier
     - Multinomial Naive Bayes
   - Used GridSearchCV for hyperparameter tuning.

5. **Model Evaluation**
   - Evaluated models using:
     - F1 Score
     - Accuracy
     - Confusion Matrix
   - **Best model:** Logistic Regression (TF-IDF), with **F1 Score â‰ˆ 0.87**

6. **Custom Review Classification**
   - Input several custom reviews to test model behavior.
   - Compared model predictions for consistency and edge-case handling.

7. **Results & Discussion**
   - Logistic Regression outperformed other models, particularly in recall.
   - TF-IDF significantly boosted classification accuracy.
   - Minor misclassifications often occurred with sarcastic or ambiguous reviews.

---

## ğŸ’¡ Insights

- **Logistic Regression** emerged as the top-performing model, striking a balance between accuracy, precision, and recall â€” particularly effective with TF-IDF features.
- **TF-IDF vectorization** proved superior to Bag-of-Words by reducing the impact of frequently occurring, less informative words and amplifying rare but sentiment-rich terms.
- **Multinomial Naive Bayes** was fast and interpretable but occasionally overfit common sentiment cues, making it less reliable on subtle or sarcastic text.
- **Random Forest** underperformed compared to expectations â€” tree-based models often struggle with high-dimensional, sparse matrices generated by vectorized text.
- **Review length and vocabulary richness** didnâ€™t always correlate with classification accuracy; short but emotionally charged phrases (e.g., â€œHated it.â€ or â€œLoved every second.â€) were easier to classify correctly.
- **Custom reviews** helped surface model edge cases. For instance, reviews with sarcasm or mixed sentiment (e.g., "I wish I could forget this masterpiece") challenged all models and sometimes led to false positives/negatives.
- The **confusion matrix** revealed that false negatives (failing to catch a bad review) were more common than false positives, which may be undesirable depending on the business goal (e.g., filtering negativity).
- Future improvements could involve:
  - Incorporating word embeddings (e.g., Word2Vec or BERT) for better contextual understanding.
  - Leveraging deep learning approaches like LSTM or transformers to handle complex language constructs.
  - Implementing sentiment-specific lexicons or ensemble strategies to improve accuracy on nuanced reviews.

---

## ğŸ› ï¸ Tools Used

- Python
- Pandas, NumPy
- scikit-learn
- NLTK
- Matplotlib, Seaborn
- Jupyter Notebook

---

## ğŸ§  Skills Applied

`Natural Language Processing (NLP)` Â· `Text Preprocessing` Â· `TF-IDF` Â· `Classification Metrics` Â· `Logistic Regression` Â· `Random Forest` Â· `Naive Bayes` Â· `Model Evaluation` Â· `Sentiment Analysis`

